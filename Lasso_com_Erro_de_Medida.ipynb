{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lasso com Erro de Medida.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "<h1><center><b>\n",
        "RELATÓRIO DO ARTIGO \"MEASUREMENT ERROR IN LASSO: IMPACT AND LIKELIHOOD BIAS CORRECTION\"\n",
        "</b></center></h1>\n",
        "\n",
        "**Disciplina:** Inferência Avançada\n",
        "\n",
        "**Aluno:**\n",
        "* João Flávio Andrade Silva\n",
        "\n",
        "**Data de entrega**: 30/06/2022\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "DCz654XGpVd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LASSO COM ERRO DE MEDIDA: IMPACTO E CORREÇÃO DE VIÉS DE PROBABILIDADE\n",
        "\n",
        "**Resumo e Introdução**\n",
        "\n",
        "A regressão com a penalidade de laço é uma ferramenta popular para realizar a redução de dimensão quando o número de covariáveis ​​é grande.\n",
        "O presente estudo aborda o impacto do erro de medição na regressão linear com a penalidade do lasso, tanto analiticamente quanto em experimentos de simulação. Um método simples de correção para erro de medição no lasso é então considerado. No limite de amostra grande, o lasso corrigido produz uma seleção de covariáveis ​​consistente de sinal sob condições muito semelhantes ao lasso com medidas perfeitas, enquanto o lasso não corrigido requer condições muito mais rigorosas na estrutura de covariância dos dados.\n",
        "\n"
      ],
      "metadata": {
        "id": "pG45iMPforcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressão Linear\n",
        "\n",
        "* Função de perda $$L(y,f(x))$$\n",
        "\n",
        "  * Utilizando função de perda quadrática, temos\n",
        "  $$L(y,f(x)) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - f(x_i))^2 $$\n",
        "  * Para função $f(x)$ linear temos $f(x) = \\beta_0 + \\sum_{j=1}^{d} x_j$ e assim\n",
        "  $$L(y,f(x) ) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 + \\sum_{j=1}^{d} \\beta_j x_{i,j} )^2 $$\n",
        "\n",
        "* Derivadas parciais da função de perda em relação aos parâmetros de interesse, no caso linear é o $\\beta$\n",
        "\n",
        "  * $ \\frac{\\partial L(y,f(x) )}{\\partial \\beta_0} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta_0) $\n",
        "  * $\\frac{\\partial L(y,f(x) )}{\\partial \\beta_j} = \\frac{-2}{n} \\sum_{i=1}^{n}  (y_i - \\beta_j x_{i,j})\n",
        "x_{i,j}, ∀ j > 0 $\n",
        "\n",
        "* Atualização das estimativas dos parâmetros\n",
        "\n",
        "  * $ \\hat \\beta_0^{k}  = \\hat \\beta_0^{k-1} - \\frac{\\partial L(y,\\hat f(x)^{k-1})}{\\partial \\hat \\beta_0^{k-1}} $\n",
        "  * $ \\hat \\beta_j^{k}  = \\hat \\beta_j^{k-1} - \\frac{\\partial L(y,\\hat f(x)^{k-1})}{\\partial \\hat \\beta_j^{k-1}}\n",
        ", ∀ j > 0 $\n",
        "\n",
        "Perceba que ao se aproximar do valor ótimo as derivadas parciais se aproximam de zero. \n",
        "\n",
        "Para compreender melhor, tomemos um caso bem simples: $\\beta x = \\beta_0$. Dessa forma, temos\n",
        "$$ \\frac{\\partial L(y,f(x) )}{\\partial \\beta_0} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta_0)$$ \n",
        "sabemos que o número de $\\beta_0$ que minimiza a função de perda é o número que a derivada parcial é zero\n",
        "$$\\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta_0) = 0 $$\n",
        "$$ \\beta_0-\\bar y = 0 $$\n",
        "\n",
        "observe que $\\bar y$ é sempre fixo, então se a diferença $\\beta_0-\\bar y$ é positiva, então é intuitivo reduzir o valor de $\\beta_0$ para aproximarmos de zero, e quando a diferença for negativa aumentar o valor de $\\beta_0$.\n",
        "\n",
        "Em $ \\beta_0-\\bar y = 0 $, você deve ter percebido que o $\\beta_0$ ótimo deve ser igual a $\\bar y$, mas isto é devido que no caso de função linear sobre o risco quadrátrico existe solução analítica, entrentanto, o método do gradiente descentende é generalista para funções não lineares e riscos diferente do quadrático - a função deve possuir derivadas parciais de primeira ordem.\n",
        "\n",
        "Gradiente descendente: a idéia é atingir um mínimo local de uma função, mas veja que escolhendo uma função que o mínimo local seja único então este será também mínimo global. Exemplos de funções em que mínimo local também é global são as funções convexas como: funções quadráticas e o inverso da verossimilhança (log-verossimilhança)."
      ],
      "metadata": {
        "id": "fY0stM3MoraD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressão Lasso\n",
        "\n",
        "\n",
        "* Função de perda $$L(y,f(x))$$\n",
        "\n",
        "  * Lasso: Função Linear sob risco quadrático com penalização norma $\\ell_1$\n",
        "  \n",
        "  $$L(y,f(x) ) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\beta_0 + \\sum_{j=1}^{d} \\beta_j x_{i,j})^2 + \\lambda \\sum_{j=1}^{d} \\vert \\beta_j \\vert $$\n",
        "\n",
        "* Derivadas parciais da função de perda em relação aos parâmetros de interesse\n",
        "\n",
        "  * $ \\frac{\\partial L(y,f(x) )}{\\partial \\beta_0} = \\frac{-1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0) $\n",
        "\n",
        "  * Se $\\beta_j > 0 $\n",
        "    * $\\frac{\\partial L(y,f(x) )}{\\partial \\beta_j} = \\frac{-1}{n} \\sum_{i=1}^{n}  (y_i - \\beta_j x_{i,j})  \n",
        "x_{i,j} + \\lambda , ∀ j > 0 $\n",
        "\n",
        "  * Se $\\beta_j <= 0 $\n",
        "    * $\\frac{\\partial L(y,f(x) )}{\\partial \\beta_j} = \\frac{-1}{n} \\sum_{i=1}^{n}  (y_i - \\beta_j x_{i,j})  \n",
        "x_{i,j} - \\lambda, ∀ j > 0 $\n",
        "\n",
        "* Atualização das estimativas dos parâmetros\n",
        "\n",
        "  * $ \\hat \\beta_0^{k}  = \\hat \\beta_0^{k-1} - \\frac{\\partial L(y,\\hat f(x)^{k-1})}{\\partial \\hat \\beta_0^{k-1}} $\n",
        "  * $ \\hat \\beta_j^{k}  = \\hat \\beta_j^{k-1} - \\frac{\\partial L(y,\\hat f(x)^{k-1})}{\\partial \\hat \\beta_j^{k-1}}\n",
        ", ∀ j > 0 $\n"
      ],
      "metadata": {
        "id": "34X6MJTDorUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressão Lasso com erro de medida nas covariáveis \n",
        "\n"
      ],
      "metadata": {
        "id": "_oAo3ncknDCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Modelo de regressão linear com erro de medida (notação matricial)\n",
        "\n",
        "$$ \\mathbf y = \\mathbf X  \\beta^0 + \\epsilon \\;\\;\\; \\textrm{e} \\;\\;\\; \\mathbf W = \\mathbf X + \\mathbf U $$\n",
        "\n",
        "com observações de $d$ covariáveis e variável resposta $ \\mathbf y \\in \\mathbb R^n$ de $n$ indivíduos. A matriz de covariáveis $\\mathbf X$ não é observada, ao invés observamos medições com ruído $\\mathbf W$. Assumimos a matriz de erros de medida $\\mathbf U \\in \\mathbb R^{n \\times d}$ com distribuição normal nas linhas com média zero e covariância $\\Sigma_{uu}$. Os erros do modelo $\\epsilon = (\\epsilon_1, \\dots, \\epsilon_n)^t$ são i.i.d. normalmente distirbuídos com média zero e variância $\\sigma^2$."
      ],
      "metadata": {
        "id": "7_4AS5icnIhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo Linear Lasso\n",
        "\n",
        "Aqui apresentamos o modelo linear Lasso sem ajuste para o problema de erro de medida, neste caso, este é chamado de *naive approach* (abordagem ingênua). \n",
        "\n",
        "  $$\\hat \\beta(\\lambda) = \\arg \\min_{\\beta} \\frac{1}{n} \\vert\\!\\vert \\mathbf y - \\mathbf W\\beta \\vert\\!\\vert_2^2 + \\lambda \\vert\\!\\vert \\beta \\vert\\!\\vert_1 $$\n",
        "\n",
        "  $$ = \\arg \\min_{\\beta} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 + \\sum_{j=1}^{d} \\beta_j w_{i,j})^2 + \\lambda \\sum_{j=1}^{d} \\vert \\beta_j \\vert $$\n",
        "\n",
        "em que $\\lambda > 0 $ é um parâmetro de regularização.\n"
      ],
      "metadata": {
        "id": "QQ7HHWtCZwSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Modelo Linear Lasso - Correção para Erro de Medida \n",
        "\n",
        "Aqui consideramos o modelo Lasso corrigido. A correção utilizada é motivada pelo fato da função de perda do *naive* Lasso ser viesada:\n",
        "$$ E[ \\vert\\!\\vert \\mathbf y - \\mathbf W\\beta \\vert\\!\\vert_2^2 \\vert \\mathbf X , \\mathbf y ] = \\vert\\!\\vert \\mathbf y - \\mathbf X \\beta \\vert\\!\\vert_2^2 \\vert + n\\beta^t\\Sigma_{uu}\\beta $$\n",
        "\n",
        "Isto sugere regularização Lasso corrigido (regularized corrected Lasso - RCL),  \n",
        "\n",
        " $$\\hat \\beta_{RCL} = \\arg \\min_{\\beta} \\frac{1}{n} \\vert\\!\\vert \\mathbf y - \\mathbf W\\beta \\vert\\!\\vert_2^2 - \\beta^t\\Sigma_{uu}\\beta + \\lambda \\vert\\!\\vert \\beta \\vert\\!\\vert_1 $$\n",
        "\n",
        "introduzida por Loh and Wainwright (2012). \n"
      ],
      "metadata": {
        "id": "5kq7L5mpu4D8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Implementação do algorítmo \n",
        "\n",
        "Nesta implementação do algorítmo de estimação dos parâmetros do modelo Linear Lasso com Correção para Erro de Medida adicionalmente supomos erros de medida independentes e com variância constante, isto é, $\\Sigma_{uu} = \\sigma_u^2Ι_d$.\n",
        "\n",
        "Logo o modelo fica resumido a \n",
        "\n",
        "$$\\hat \\beta_{RCL} = \\arg \\min_{\\beta} \\frac{1}{n} \\vert\\!\\vert \\mathbf y - \\mathbf W\\beta \\vert\\!\\vert_2^2 - \\beta^t\\sigma_u^2Ι_d\\beta + \\lambda \\vert\\!\\vert \\beta \\vert\\!\\vert_1 $$\n",
        "\n",
        "e nossa função de perda pode ser reescrita na forma\n",
        " \n",
        "$$ L(y,f(w) )  = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 + \\sum_{j=1}^{d} \\beta_j w_{i,j})^2 - \\sigma_u^2 \\sum_{j=1}^{d}  \\beta_j^2 + \\lambda \\sum_{j=1}^{d} \\vert \\beta_j \\vert $$\n",
        "\n",
        "Temos as Derivadas parciais da função de perda em relação aos parâmetros de interesse\n",
        "\n",
        "  * $ \\frac{\\partial L(y,f(w) )}{\\partial \\beta_0} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\beta_0) $\n",
        "\n",
        "  * Se $\\beta_j > 0 $\n",
        "    * $\\frac{\\partial L(y,f(w) )}{\\partial \\beta_j} = \\frac{-2}{n} \\sum_{i=1}^{n}  (y_i - \\beta_j w_{i,j})  \n",
        "w_{i,j} - 2\\sigma_u^2\\beta_j + \\lambda , ∀ j > 0 $\n",
        "\n",
        "  * Se $\\beta_j <= 0 $\n",
        "    * $\\frac{\\partial L(y,f(w) )}{\\partial \\beta_j} = \\frac{-2}{n} \\sum_{i=1}^{n}  (y_i - \\beta_j w_{i,j})  \n",
        "w_{i,j} - 2\\sigma_u^2\\beta_j - \\lambda , ∀ j > 0 $\n",
        "\n",
        "* Atualização das estimativas dos parâmetros\n",
        "\n",
        "  * $ \\hat \\beta_0^{k}  = \\hat \\beta_0^{k-1} - \\frac{\\partial L(y,\\hat f(x)^{k-1})}{\\partial \\hat \\beta_0^{k-1}} $\n",
        "  * $ \\hat \\beta_j^{k}  = \\hat \\beta_j^{k-1} - \\frac{\\partial L(y,\\hat f(x)^{k-1})}{\\partial \\hat \\beta_j^{k-1}}\n",
        ", ∀ j > 0 $\n",
        "  "
      ],
      "metadata": {
        "id": "GoccyLK4vCdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressão Elastic Net\n",
        "\n",
        "\n",
        "* Função de perda $$L(y,f(x))$$\n",
        "\n",
        "  * Lasso: Função Linear sob risco quadrático com penalização norma $\\ell_2$ e $\\ell_1$\n",
        "  \n",
        "  $$L(y,f(x) ) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\beta_0 + \\sum_{j=1}^{d} \\beta_j x_{i,j})^2 + \\lambda \\left(\\frac{(1-\\alpha)}{2} \\sum_{j=1}^{d}  \\beta_j^2 + \\alpha \\sum_{j=1}^{d} \\vert \\beta_j \\vert \\right) $$\n",
        "\n",
        "* Derivadas parciais da função de perda em relação aos parâmetros de interesse\n",
        "\n",
        "  * $ \\frac{\\partial L(y,f(x) )}{\\partial \\beta_0} = \\frac{-1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0) $\n",
        "\n",
        "  * Se $\\beta_j > 0 $\n",
        "    * $\\frac{\\partial L(y,f(x) )}{\\partial \\beta_j} = \\frac{-1}{n} \\sum_{i=1}^{n}  (y_i - \\beta_j x_{i,j})  \n",
        "x_{i,j} + \\lambda (1 - \\alpha)\\beta_j + \\lambda \\alpha , ∀ j > 0 $\n",
        "\n",
        "  * Se $\\beta_j <= 0 $\n",
        "    * $\\frac{\\partial L(y,f(x) )}{\\partial \\beta_j} = \\frac{-1}{n} \\sum_{i=1}^{n}  (y_i - \\beta_j x_{i,j})  \n",
        "x_{i,j} + \\lambda (1 - \\alpha)\\beta_j - \\lambda \\alpha, ∀ j > 0 $\n",
        "\n",
        "* Atualização das estimativas dos parâmetros\n",
        "\n",
        "  * $ \\hat \\beta_0^{k}  = \\hat \\beta_0^{k-1} - \\frac{\\partial L(y,\\hat f(x)^{k-1})}{\\partial \\hat \\beta_0^{k-1}} $\n",
        "  * $ \\hat \\beta_j^{k}  = \\hat \\beta_j^{k-1} - \\frac{\\partial L(y,\\hat f(x)^{k-1})}{\\partial \\hat \\beta_j^{k-1}}\n",
        ", ∀ j > 0 $\n"
      ],
      "metadata": {
        "id": "gI9bnPN3uoYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementações dos algorítmos \n",
        "\n",
        "Para cada um dos modelos descritos anteriormente implementei um algoritmo utilizando linguagem Python (com auxílio apenas do pacote Numpy) e programação orientada objeto, em todos os algorítmos o método de otimização empregado foi o Gradiente Descente.\n",
        "\n",
        "Objetos criados:\n",
        "* LinearRegression(): Modelo de Regressão Linear\n",
        "* Lasso(): Modelo de Regressão Linear Lasso \n",
        "* ElasticNet(): Modelo de Regressão Linear Elastic Net\n",
        "* LassoME(): Modelo de Regressão Linear Lasso com ajuste para Erro de Medida\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v5OV6WEXu0aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression():\n",
        "  \n",
        "  def __init__(self,intercept=True, max_iter=1000,tol=0.001,learning_rate=0.1,random_state=None):\n",
        "    self.intercept = intercept\n",
        "    self.max_iter = max_iter\n",
        "    self.tol = tol    \n",
        "    self.learning_rate = learning_rate\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def predict(self,X):\n",
        "    return self.intercept_ + X.dot(self.coef_) \n",
        "\n",
        "  def fit(self,X,y):\n",
        "    import numpy as np\n",
        "\n",
        "    np.random.seed(self.random_state)\n",
        "    self.n_, self.d_ = X.shape\n",
        "    self.coef_ = np.random.normal(0,1,self.d_)\n",
        "    self.intercept_ = 0\n",
        "\n",
        "    # gradiente descendente \n",
        "    for i in range(1,self.max_iter+1) : \n",
        "      predictions = self.predict(X)    \n",
        "      d_coef = np.array( [ (-2/self.n_)*(y - predictions).dot(X[:,k]) \n",
        "                      for k in range(self.d_) ])\n",
        "      self.coef_ -= d_coef*self.learning_rate\n",
        "      if self.intercept == True:\n",
        "        d_int = (-2/self.n_)*np.sum(y - predictions)\n",
        "        self.intercept_ -= d_int*self.learning_rate\n",
        "\n",
        "      c_tol = np.max(np.abs(d_coef*self.learning_rate))/np.max(np.abs(self.coef_))\n",
        "      if c_tol <= self.tol:         \n",
        "        self.n_iter_ = i \n",
        "        break\n",
        "    if i == self.max_iter:\n",
        "      print('Máximo de iterações atingido')  \n",
        "    return self    \n",
        "\n",
        "class Lasso():\n",
        "  \n",
        "  def __init__(self,intercept=True,Lambda=1,max_iter=1000,tol=0.001,learning_rate=0.1,random_state=None):\n",
        "    self.intercept = intercept \n",
        "    self.Lambda = Lambda\n",
        "    self.max_iter = max_iter\n",
        "    self.tol = tol    \n",
        "    self.learning_rate = learning_rate\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def predict(self,X):\n",
        "    return self.intercept_ + X.dot(self.coef_) \n",
        "\n",
        "  def fit(self,X,y):\n",
        "    import numpy as np\n",
        "\n",
        "    np.random.seed(self.random_state)\n",
        "    self.n_, self.d_ = X.shape\n",
        "    self.coef_ = np.random.normal(0,1,self.d_)\n",
        "    self.intercept_ = 0\n",
        "\n",
        "    # gradiente descendente \n",
        "    for i in range(1,self.max_iter+1) : \n",
        "      predictions = self.predict(X)    \n",
        "      d_coef = np.array( [ np.where( self.coef_[k] > 0 \n",
        "                          , (-1/self.n_)*(y - predictions).dot(X[:,k]) + self.Lambda  \n",
        "                          , (-1/self.n_)*(y - predictions).dot(X[:,k]) - self.Lambda ) \n",
        "                      for k in range(self.d_) ])\n",
        "      self.coef_ -= d_coef*self.learning_rate\n",
        "      if self.intercept == True:\n",
        "        d_int = (-1/self.n_)*np.sum(y - predictions)\n",
        "        self.intercept_ -= d_int*self.learning_rate\n",
        "\n",
        "      c_tol = np.max(np.abs(d_coef*self.learning_rate))/np.max(np.abs(self.coef_))\n",
        "      if c_tol <= self.tol:         \n",
        "        self.n_iter_ = i \n",
        "        break\n",
        "    if i == self.max_iter:\n",
        "      print('Máximo de iterações atingido')  \n",
        "    return self        \n",
        "\n",
        "class ElasticNet():\n",
        "  \n",
        "  def __init__(self,intercept=True,Lambda=1,alpha=1,max_iter=1000,tol=0.001,learning_rate=0.1,random_state=None):\n",
        "    self.intercept = intercept \n",
        "    self.Lambda = Lambda\n",
        "    self.alpha = alpha    \n",
        "    self.max_iter = max_iter\n",
        "    self.tol = tol    \n",
        "    self.learning_rate = learning_rate\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def predict(self,X):\n",
        "    return self.intercept_ + X.dot(self.coef_) \n",
        "\n",
        "  def fit(self,X,y):\n",
        "    import numpy as np\n",
        "\n",
        "    np.random.seed(self.random_state)\n",
        "    self.n_, self.d_ = X.shape\n",
        "    self.coef_ = np.random.normal(0,1,self.d_)\n",
        "    self.intercept_ = 0\n",
        "\n",
        "    # gradiente descendente \n",
        "    for i in range(1,self.max_iter+1) : \n",
        "      predictions = self.predict(X)    \n",
        "      d_coef = np.array( [ np.where( self.coef_[k] > 0 \n",
        "                          , (-1/self.n_)*(y - predictions).dot(X[:,k]) + self.Lambda*(1-self.alpha)*self.coef_[k] + self.Lambda*self.alpha  \n",
        "                          , (-1/self.n_)*(y - predictions).dot(X[:,k]) + self.Lambda*(1-self.alpha)*self.coef_[k] - self.Lambda*self.alpha ) \n",
        "                      for k in range(self.d_) ])\n",
        "      self.coef_ -= d_coef*self.learning_rate\n",
        "      if self.intercept == True:\n",
        "        d_int = (-1/self.n_)*np.sum(y - predictions)\n",
        "        self.intercept_ -= d_int*self.learning_rate\n",
        "\n",
        "      c_tol = np.max(np.abs(d_coef*self.learning_rate))/np.max(np.abs(self.coef_))\n",
        "      if c_tol <= self.tol:         \n",
        "        self.n_iter_ = i \n",
        "        break\n",
        "    if i == self.max_iter:\n",
        "      print('Máximo de iterações atingido')  \n",
        "    return self        \n",
        "\n",
        "# class LassoME():\n",
        "  \n",
        "#   def __init__(self,intercept=True,Lambda=1,sigma=1,max_iter=1000,tol=0.001,learning_rate=0.1,random_state=None):\n",
        "#     self.intercept = intercept \n",
        "#     self.Lambda = Lambda\n",
        "#     self.sigma = sigma    \n",
        "#     self.max_iter = max_iter\n",
        "#     self.tol = tol    \n",
        "#     self.learning_rate = learning_rate\n",
        "#     self.random_state = random_state\n",
        "\n",
        "#   def predict(self,X):\n",
        "#     return self.intercept_ + X.dot(self.coef_) \n",
        "\n",
        "#   def fit(self,X,y):\n",
        "#     import numpy as np\n",
        "\n",
        "#     np.random.seed(self.random_state)\n",
        "#     self.n_, self.d_ = X.shape\n",
        "#     self.coef_ = np.random.normal(0,1,self.d_)\n",
        "#     self.intercept_ = 0\n",
        "\n",
        "#     # gradiente descendente \n",
        "#     for i in range(1,self.max_iter+1):\n",
        "#       predictions = self.predict(X)    \n",
        "#       # d_int = (-1/self.n_)*np.sum(y - predictions)\n",
        "#       # d_coef = np.array( [ np.where( self.coef_[k] > 0 \n",
        "#       #                     , (-1/self.n_)*(y - predictions).dot(X[:,k]) -(self.sigma)*self.coef_[k] + self.Lambda  \n",
        "#       #                     , (-1/self.n_)*(y - predictions).dot(X[:,k]) -(self.sigma)*self.coef_[k] - self.Lambda ) \n",
        "#       #                 for k in range(self.d_) ])\n",
        "#       d_coef = np.array( [ np.where( self.coef_[k] > 0 \n",
        "#                           , (-2/self.n_)*(y - predictions).dot(X[:,k]) -2*(self.sigma)*self.coef_[k] + self.Lambda  \n",
        "#                           , (-2/self.n_)*(y - predictions).dot(X[:,k]) -2*(self.sigma)*self.coef_[k] - self.Lambda ) \n",
        "#                       for k in range(self.d_) ])\n",
        "#       self.coef_ -= d_coef*self.learning_rate\n",
        "#       if self.intercept == True:\n",
        "#         d_int = (-2/self.n_)*np.sum(y - predictions)\n",
        "#         self.intercept_ -= d_int*self.learning_rate\n",
        "\n",
        "#       c_tol = np.max(np.abs(d_coef*self.learning_rate))/np.max(np.abs(self.coef_))\n",
        "#       if c_tol <= self.tol:         \n",
        "#         self.n_iter_ = i \n",
        "#         break\n",
        "#     if i == self.max_iter:\n",
        "#       print('Máximo de iterações atingido')  \n",
        "#     return self        \n",
        "\n",
        "class LassoME():\n",
        "  \n",
        "  def __init__(self,intercept=True,Lambda=1,sigma=1,max_iter=1000,tol=0.001,learning_rate=0.1,random_state=None):\n",
        "    self.intercept = intercept \n",
        "    self.Lambda = Lambda\n",
        "    self.sigma = sigma    \n",
        "    self.max_iter = max_iter\n",
        "    self.tol = tol    \n",
        "    self.learning_rate = learning_rate\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def predict(self,X):\n",
        "    return self.intercept_ + X.dot(self.coef_) \n",
        "\n",
        "  def fit(self,X,y):\n",
        "    import numpy as np\n",
        "\n",
        "    np.random.seed(self.random_state)\n",
        "    self.n_, self.d_ = X.shape\n",
        "    self.coef_ = np.random.normal(0,1,self.d_)\n",
        "    self.intercept_ = 0\n",
        "\n",
        "    xx = np.matmul( np.matrix.transpose(X), X)/n\n",
        "    xy = np.matmul( np.matrix.transpose(X), y)/n\n",
        "\n",
        "    # gradiente descendente \n",
        "    for i in range(1,self.max_iter+1):\n",
        "      predictions = self.predict(X)    \n",
        "      d_coef_aux = self.coef_.dot(xx - np.diag( np.repeat(self.sigma, self.d_) ) ) - xy\n",
        "      d_coef = np.array( [ np.where( self.coef_[k] > 0 \n",
        "                          , d_coef_aux[k] + self.Lambda  \n",
        "                          , d_coef_aux[k] - self.Lambda ) \n",
        "                      for k in range(self.d_) ])\n",
        "      self.coef_ -= d_coef*self.learning_rate\n",
        "\n",
        "      c_tol = np.max(np.abs(d_coef*self.learning_rate))/np.max(np.abs(self.coef_))\n",
        "      if c_tol <= self.tol:         \n",
        "        self.n_iter_ = i \n",
        "        break\n",
        "    if i == self.max_iter:\n",
        "      print('Máximo de iterações atingido')  \n",
        "    return self            "
      ],
      "metadata": {
        "id": "6J4aYsxxQ5Sx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimento de Simulação\n",
        "\n",
        "Nesta seção tentei replicar parcialmente o experimento realizado no item *6.1 Linear regression* do artigo estudado [1], para $s_0 = 5$ e $\\sigma_U^2=0.2$, utilizando os algoritmos aqui implementados."
      ],
      "metadata": {
        "id": "4VdsPJk82jIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "n = 100\n",
        "d = 500 \n",
        "s0 = 5 \n",
        "\n",
        "B0 = np.concatenate( (np.random.normal(0,2**2,s0), np.zeros(d-s0) ), axis=None )\n",
        "e = np.random.normal(0,0.01,n)\n",
        "X = np.random.normal(0,1,n*d).reshape(n,d)\n",
        "y = X.dot(B0) + e\n",
        "U = np.random.normal(0,0.2,n*d).reshape(n,d)\n",
        "W = X + U"
      ],
      "metadata": {
        "id": "6SH8TbmU1H_E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning dos hiperparâmetros"
      ],
      "metadata": {
        "id": "DtJz2MpeD82g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_dict = {\"Lambda\":[],\"EQM\":[]}\n",
        "lasso_me_dict = {\"Lambda\":[],\"EQM\":[]}\n",
        "\n",
        "for Lambda in np.arange(0.05,0.3,0.05):\n",
        "  lasso_err = []\n",
        "  lasso_me_err = []\n",
        "  kf = KFold(n_splits=5)\n",
        "  for train_index, test_index in kf.split(W):\n",
        "    W_train, W_test = W[train_index], W[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    lasso = Lasso(intercept = False, Lambda=Lambda,max_iter=1_000,learning_rate=0.01,random_state=1).fit(W_train,y_train)\n",
        "    lasso_me = LassoME(intercept = False, Lambda=Lambda, sigma = 0.2,max_iter=1000,learning_rate=0.01,random_state=1).fit(W_train,y_train)    \n",
        "    \n",
        "    lasso_err.append(np.mean((lasso.predict(W_test) - y_test)**2))\n",
        "    lasso_me_err.append(np.mean((lasso_me.predict(W_test) - y_test)**2))\n",
        "\n",
        "  lasso_dict[\"Lambda\"].append(Lambda)\n",
        "  lasso_dict[\"EQM\"].append( np.mean(lasso_err) )\n",
        "  lasso_me_dict[\"Lambda\"].append(Lambda)\n",
        "  lasso_me_dict[\"EQM\"].append( np.mean(lasso_me_err) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjdjGYEJ8-YG",
        "outputId": "cc5142eb-fe56-4f8f-bd4b-f436f04b8046"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "elastic_dict = {\"alpha\":[],\"Lambda\":[],\"EQM\":[]}\n",
        "\n",
        "for alpha in np.arange(0.05,0.95,0.2):\n",
        "  for Lambda in np.arange(0.05,0.3,0.05):\n",
        "    elastic_err = []\n",
        "    kf = KFold(n_splits=5)\n",
        "    for train_index, test_index in kf.split(W):\n",
        "      W_train, W_test = W[train_index], W[test_index]\n",
        "      y_train, y_test = y[train_index], y[test_index]\n",
        "      \n",
        "      elastic = ElasticNet(intercept = False, Lambda=Lambda, alpha = alpha,random_state=1).fit(W_train,y_train)\n",
        "\n",
        "      elastic_err.append(np.mean((elastic.predict(W_test) - y_test)**2))\n",
        "    \n",
        "    elastic_dict[\"alpha\"].append(alpha)\n",
        "    elastic_dict[\"Lambda\"].append(Lambda)\n",
        "    elastic_dict[\"EQM\"].append( np.mean(elastic_err) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGKHKwJK9DlI",
        "outputId": "20d51458-13ea-484c-bc67-4bf9c329757b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n",
            "Máximo de iterações atingido\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display( pd.DataFrame(lasso_dict).assign(Model='Lasso').sort_values('EQM')\n",
        "        ,pd.DataFrame(lasso_me_dict).assign(Model='LassoME').sort_values('EQM')\n",
        "        ,pd.DataFrame(elastic_dict).assign(Model='ElasticNet').sort_values('EQM') )"
      ],
      "metadata": {
        "id": "Lo1tB-UeIBMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_dict = {\"Model\":[],\"L1\":[],\"L2\":[],\"EQM\":[]}\n",
        "\n",
        "for i in range(100):\n",
        "  B0 = np.concatenate( (np.random.normal(0,2**2,s0), np.zeros(d-s0) ), axis=None )\n",
        "  e = np.random.normal(0,0.01,n)\n",
        "  X = np.random.normal(0,1,n*d).reshape(n,d)\n",
        "  y = X.dot(B0) + e\n",
        "  U = np.random.normal(0,0.2,n*d).reshape(n,d)\n",
        "  W = X + U\n",
        "\n",
        "  lasso = Lasso(intercept = False, Lambda=0.25,max_iter=1_000,learning_rate=0.01,random_state=1).fit(W,y)\n",
        "  lasso_me = LassoME(intercept = False, Lambda=0.25, sigma = 0.2,max_iter=1000,learning_rate=0.01,random_state=1).fit(W,y)    \n",
        "  elastic = ElasticNet(intercept = False, Lambda=0.15, alpha = 0.85,random_state=1).fit(W,y)\n",
        "\n",
        "  results_dict[\"Model\"].append('Lasso')\n",
        "  results_dict[\"L1\"].append(np.linalg.norm(lasso.coef_ - B0, ord=1)/d)\n",
        "  results_dict[\"L2\"].append(np.linalg.norm(lasso.coef_ - B0, ord=2)/d)\n",
        "  results_dict[\"EQM\"].append(np.mean((lasso.predict(W) - y)**2))\n",
        "\n",
        "  results_dict[\"Model\"].append('LassoME')\n",
        "  results_dict[\"L1\"].append(np.linalg.norm(lasso_me.coef_ - B0, ord=1)/d)\n",
        "  results_dict[\"L2\"].append(np.linalg.norm(lasso_me.coef_ - B0, ord=2)/d)\n",
        "  results_dict[\"EQM\"].append(np.mean((lasso_me.predict(W) - y)**2))\n",
        "\n",
        "  results_dict[\"Model\"].append('ElasticNet')\n",
        "  results_dict[\"L1\"].append(np.linalg.norm(elastic.coef_ - B0, ord=1)/d)\n",
        "  results_dict[\"L2\"].append(np.linalg.norm(elastic.coef_ - B0, ord=2)/d)\n",
        "  results_dict[\"EQM\"].append(np.mean((elastic.predict(W) - y)**2))"
      ],
      "metadata": {
        "id": "jPxNx8rG-075"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(results_dict).groupby('Model').agg(['mean','std'])"
      ],
      "metadata": {
        "id": "kDHmE_3aJt8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referências Principais\n",
        "\n",
        "[1] SØRENSEN, Øystein; FRIGESSI, Arnoldo; THORESEN, Magne. Measurement error in LASSO: Impact and likelihood bias correction. Statistica sinica, p. 809-829, 2015.\n",
        "\n",
        "[2] FRIEDMAN, Jerome; HASTIE, Trevor; TIBSHIRANI, Rob. Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, v. 33, n. 1, p. 1, 2010. Disponível em: <https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf>\n",
        "\n",
        "[3] LOH, Po-Ling; WAINWRIGHT, Martin J. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Advances in neural information processing systems, v. 24, 2011.\n",
        "\n",
        "[4] Gradiente Descentende. Método do Gradiente Descendente. Disponível em: <https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html>\n",
        "\n"
      ],
      "metadata": {
        "id": "yNcP_G002YWD"
      }
    }
  ]
}